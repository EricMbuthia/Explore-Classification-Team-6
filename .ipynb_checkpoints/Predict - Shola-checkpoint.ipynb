{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f04903e",
   "metadata": {},
   "source": [
    "# Classification Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "We, **Team 6**, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: EDSA - Climate Change Belief Analysis 2022\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, you are challenged to create a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0994b86",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1ad4a",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89472004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\\\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from textatistic import Textatistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52d633",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion of libraries that will be used\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a4678",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `train.csv` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff5193",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msg = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing web URL\n",
    "#COnsider removing twitter handles\n",
    "\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_msg['message'] = all_msg['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting message column to lower case\n",
    "\n",
    "all_msg['message'] = all_msg['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "all_msg['message'] = all_msg['message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = TreebankWordTokenizer()\n",
    "all_msg['tokens'] = all_msg['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88744485",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]    \n",
    "\n",
    "all_msg['lemma'] = all_msg['tokens'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8affdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_info = all_msg.copy()\n",
    "def count_words(word):\n",
    "    word_list = word.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "added_info['word_count'] = added_info['message'].apply(count_words)\n",
    "\n",
    "def avg_word_length(word):\n",
    "    string_length =  len(word)\n",
    "    word_list = word.split(\" \")\n",
    "    word_count = len(word_list)\n",
    "    return string_length/word_count\n",
    "\n",
    "added_info['avg_word_length'] = added_info['message'].apply(avg_word_length)\n",
    "\n",
    "def count_citations(word):\n",
    "    word_list = word.split(\" \")\n",
    "    count = 0 \n",
    "    \n",
    "    for word in word_list:\n",
    "        if word == \"urlweb\":\n",
    "            count = count + 1\n",
    "            \n",
    "    return count            \n",
    "\n",
    "added_info['citations'] = added_info['message'].apply(count_citations)\n",
    "\n",
    "def count_retweets(word):\n",
    "    word_list = word.split(\" \")\n",
    "    rt_count = 0\n",
    "    for word in word_list:\n",
    "        if word == 'rt':\n",
    "            rt_count = rt_count + 1\n",
    "    return rt_count\n",
    "\n",
    "added_info['rt_count'] = added_info['message'].apply(count_retweets)\n",
    "\n",
    "def list_to_string(post):\n",
    "    return ' '.join(post)\n",
    "\n",
    "added_info['lemma'] = added_info['lemma'].apply(list_to_string)\n",
    "\n",
    "tf_vect = TfidfVectorizer()\n",
    "X_train_vect = tf_vect.fit_transform(added_info.lemma)\n",
    "\n",
    "added_info['lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122dedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_vect.toarray())\n",
    "\n",
    "X_train_df['word_count'] = added_info['word_count']\n",
    "X_train_df['avg_word_length'] = added_info['avg_word_length']\n",
    "X_train_df['citations'] = added_info['citations']\n",
    "X_train_df['rt_count'] = added_info['rt_count']\n",
    "\n",
    "\n",
    "y_train_df = added_info.sentiment\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_train_df.to_numpy(), y_train_df.to_numpy(), test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = added_info[added_info.sentiment == 1]\n",
    "one = tf_vect.transform(one.lemma)\n",
    "one = one.toarray()\n",
    "one = pd.DataFrame(one)\n",
    "one_vect = pd.DataFrame(one.mean()).T.to_numpy()[0]\n",
    "one_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ = added_info[added_info.sentiment == -1]\n",
    "one_ = tf_vect.transform(one_.lemma)\n",
    "one_ = one_.toarray()\n",
    "one_ = pd.DataFrame(one_)\n",
    "one__vect = pd.DataFrame(one_.mean()).T.to_numpy()[0]\n",
    "one__vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = added_info[added_info.sentiment == 0]\n",
    "zero = tf_vect.transform(zero.lemma)\n",
    "zero = zero.toarray()\n",
    "zero = pd.DataFrame(zero)\n",
    "zero_vect = pd.DataFrame(zero.mean()).T.to_numpy()[0]\n",
    "zero_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a794411",
   "metadata": {},
   "outputs": [],
   "source": [
    "two = added_info[added_info.sentiment == 2]\n",
    "two = tf_vect.transform(two.lemma)\n",
    "two = two.toarray()\n",
    "two = pd.DataFrame(two)\n",
    "two_vect = pd.DataFrame(two.mean()).T.to_numpy()[0]\n",
    "two_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f904760",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = all_msg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['message'] = df_test['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "df_test['message'] = df_test['message'].str.lower()\n",
    "df_test['message'] = df_test['message'].apply(remove_punctuation)\n",
    "df_test['tokens'] = df_test['message'].apply(tokeniser.tokenize)\n",
    "df_test['lemma'] = df_test['tokens'].apply(mbti_lemma, args=(lemmatizer, ))\n",
    "newtestdf = df_test.copy()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fb753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(post):\n",
    "    return ' '.join(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msg = newdf.copy()\n",
    "df_test = newtestdf.copy()\n",
    "\n",
    "all_msg['lemma'] = all_msg['lemma'].apply(list_to_string)\n",
    "df_test['lemma'] = df_test['lemma'].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msg.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfe1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['lemma']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4438ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msg['lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ac2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = betterVect.fit_transform(all_msg['lemma'])\n",
    "\n",
    "\n",
    "X_test = betterVect.transform(df_test['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b35353",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4dfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = tf_vect.transform(df_test['lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593568c0",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more classification models that is/are able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d263979",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X_train2, y_train2)\n",
    "logreg2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_poly = SVC(kernel=\"poly\")\n",
    "svc_poly.fit(X_train, y_train)\n",
    "svc_poly.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e77d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_poly2 = SVC(kernel=\"poly\")\n",
    "svc_poly2.fit(X_train2, y_train2)\n",
    "svc_poly2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_lm = SVC(kernel=\"linear\")\n",
    "svc_lm.fit(X_train, y_train)\n",
    "svc_lm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9857779",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_lm2 = SVC(kernel=\"linear\")\n",
    "svc_lm2.fit(X_train2, y_train2)\n",
    "svc_lm2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdBoost = AdaBoostClassifier()\n",
    "AdBoost.fit(X_train, y_train)\n",
    "AdBoost.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdBoost2 = AdaBoostClassifier()\n",
    "AdBoost2.fit(X_train2, y_train2)\n",
    "AdBoost2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "RF.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF2 = RandomForestClassifier()\n",
    "RF2.fit(X_train2, y_train2)\n",
    "RF2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "MNB.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB2 = MultinomialNB()\n",
    "MNB2.fit(X_train2, y_train2)\n",
    "MNB2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c73e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [(\"LM\", LogisticRegression()), (\"RF\", RandomForestClassifier()), (\"SVR\", SVC(kernel=\"linear\"))]\n",
    "\n",
    "#meta_learner = LogisticRegression()\n",
    "\n",
    "\n",
    "#stack_clf = StackingClassifier(estimators=models, final_estimator=meta_learner)\n",
    "#stack_clf.fit(X_train2, y_train2)\n",
    "#stack_clf.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb8078",
   "metadata": {},
   "source": [
    "### Emsemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc_lm2.predict(X_test)\n",
    "\n",
    "\n",
    "df_CSV = pd.DataFrame({\"tweetid\": df_test['tweetid'].values,\n",
    "                   \"sentiment\": predictions,\n",
    "                  })\n",
    "\n",
    "df_CSV.to_csv(\"Team6_sample_MNB_ProT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036b84f",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee55a1d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c112a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
