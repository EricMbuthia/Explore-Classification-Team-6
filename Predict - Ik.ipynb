{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f04903e",
   "metadata": {},
   "source": [
    "# Classification Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "We, **Team 6**, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: EDSA - Climate Change Belief Analysis 2022\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, you are challenged to create a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0994b86",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1ad4a",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89472004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textatistic in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: pyhyphen>=2.0.5 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from textatistic) (4.0.3)\n",
      "Requirement already satisfied: setuptools>=52.0 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from pyhyphen>=2.0.5->textatistic) (58.0.4)\n",
      "Requirement already satisfied: wheel>=0.36.0 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from pyhyphen>=2.0.5->textatistic) (0.37.0)\n",
      "Requirement already satisfied: requests>=2.25 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from pyhyphen>=2.0.5->textatistic) (2.26.0)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from pyhyphen>=2.0.5->textatistic) (1.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ikechukwu/anaconda3/lib/python3.9/site-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63f5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\\\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from textatistic import Textatistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52d633",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion of libraries that will be used\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a4678",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `train.csv` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daeb9e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e669bb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @RawStory: Researchers say we have three years to act on climate change before it’s too late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff5193",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fb9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msg = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437e2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing web URL\n",
    "#COnsider removing twitter handles\n",
    "\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_msg['message'] = all_msg['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff8c934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795a22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting message column to lower case\n",
    "\n",
    "all_msg['message'] = all_msg['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfaa0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "all_msg['message'] = all_msg['message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffc492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = TreebankWordTokenizer()\n",
    "all_msg['tokens'] = all_msg['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88744485",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]    \n",
    "\n",
    "all_msg['lemma'] = all_msg['tokens'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "675cd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8affdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89bc78d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        polyscimajor epa chief doesnt think carbon dio...\n",
       "1        it not like we lack evidence of anthropogenic ...\n",
       "2        rt rawstory researcher say we have three year ...\n",
       "3        todayinmaker wired 2016 wa a pivotal year in t...\n",
       "4        rt soynoviodetodas it 2016 and a racist sexist...\n",
       "                               ...                        \n",
       "15814    rt ezlusztig they took down the material on gl...\n",
       "15815    rt washingtonpost how climate change could be ...\n",
       "15816    notiven rt nytimesworld what doe trump actuall...\n",
       "15817    rt sara8smiles hey liberal the climate change ...\n",
       "15818    rt chetcannon kurteichenwalds climate change e...\n",
       "Name: lemma, Length: 15819, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_info = all_msg.copy()\n",
    "def count_words(word):\n",
    "    word_list = word.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "added_info['word_count'] = added_info['message'].apply(count_words)\n",
    "\n",
    "def avg_word_length(word):\n",
    "    string_length =  len(word)\n",
    "    word_list = word.split(\" \")\n",
    "    word_count = len(word_list)\n",
    "    return string_length/word_count\n",
    "\n",
    "added_info['avg_word_length'] = added_info['message'].apply(avg_word_length)\n",
    "\n",
    "def count_citations(word):\n",
    "    word_list = word.split(\" \")\n",
    "    count = 0 \n",
    "    \n",
    "    for word in word_list:\n",
    "        if word == \"urlweb\":\n",
    "            count = count + 1\n",
    "            \n",
    "    return count            \n",
    "\n",
    "added_info['citations'] = added_info['message'].apply(count_citations)\n",
    "\n",
    "def count_retweets(word):\n",
    "    word_list = word.split(\" \")\n",
    "    rt_count = 0\n",
    "    for word in word_list:\n",
    "        if word == 'rt':\n",
    "            rt_count = rt_count + 1\n",
    "    return rt_count\n",
    "\n",
    "added_info['rt_count'] = added_info['message'].apply(count_retweets)\n",
    "\n",
    "def list_to_string(post):\n",
    "    return ' '.join(post)\n",
    "\n",
    "added_info['lemma'] = added_info['lemma'].apply(list_to_string)\n",
    "\n",
    "tf_vect = TfidfVectorizer()\n",
    "X_train_vect = tf_vect.fit_transform(added_info.lemma)\n",
    "\n",
    "added_info['lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6945c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_df = pd.DataFrame(X_train_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "122dedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_df['word_count'] = added_info['word_count']\n",
    "#X_train_df['avg_word_length'] = added_info['avg_word_length']\n",
    "#X_train_df['citations'] = added_info['citations']\n",
    "#X_train_df['rt_count'] = added_info['rt_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc26bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = added_info.sentiment\n",
    "#X_train2 = X_train_df.to_numpy()\n",
    "#y_train2 = y_train_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one = added_info[added_info.sentiment == 1]\n",
    "# one = tf_vect.transform(one.lemma)\n",
    "# one = one.toarray()\n",
    "# one = pd.DataFrame(one)\n",
    "# one_vect = pd.DataFrame(one.mean()).T.to_numpy()[0]\n",
    "# one_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_ = added_info[added_info.sentiment == -1]\n",
    "# one_ = tf_vect.transform(one_.lemma)\n",
    "# one_ = one_.toarray()\n",
    "# one_ = pd.DataFrame(one_)\n",
    "# one__vect = pd.DataFrame(one_.mean()).T.to_numpy()[0]\n",
    "# one__vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero = added_info[added_info.sentiment == 0]\n",
    "# zero = tf_vect.transform(zero.lemma)\n",
    "# zero = zero.toarray()\n",
    "# zero = pd.DataFrame(zero)\n",
    "# zero_vect = pd.DataFrame(zero.mean()).T.to_numpy()[0]\n",
    "# zero_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a794411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two = added_info[added_info.sentiment == 2]\n",
    "# two = tf_vect.transform(two.lemma)\n",
    "# two = two.toarray()\n",
    "# two = pd.DataFrame(two)\n",
    "# two_vect = pd.DataFrame(two.mean()).T.to_numpy()[0]\n",
    "# two_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f904760",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = all_msg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1773dcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[europe, will, now, be, looking, to, china, to...</td>\n",
       "      <td>[europe, will, now, be, looking, to, china, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[combine, this, with, the, polling, of, staffe...</td>\n",
       "      <td>[combine, this, with, the, polling, of, staffe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  europe will now be looking to china to make su...   169760   \n",
       "1  combine this with the polling of staffers re c...    35326   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [europe, will, now, be, looking, to, china, to...   \n",
       "1  [combine, this, with, the, polling, of, staffe...   \n",
       "\n",
       "                                               lemma  \n",
       "0  [europe, will, now, be, looking, to, china, to...  \n",
       "1  [combine, this, with, the, polling, of, staffe...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['message'] = df_test['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "df_test['message'] = df_test['message'].str.lower()\n",
    "df_test['message'] = df_test['message'].apply(remove_punctuation)\n",
    "df_test['tokens'] = df_test['message'].apply(tokeniser.tokenize)\n",
    "df_test['lemma'] = df_test['tokens'].apply(mbti_lemma, args=(lemmatizer, ))\n",
    "newtestdf = df_test.copy()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64fb753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(post):\n",
    "    return ' '.join(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efe8d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msg = newdf.copy()\n",
    "df_test = newtestdf.copy()\n",
    "\n",
    "all_msg['lemma'] = all_msg['lemma'].apply(list_to_string)\n",
    "df_test['lemma'] = df_test['lemma'].apply(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c7ac2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = betterVect.fit_transform(all_msg['lemma'])\n",
    "y_train = df.sentiment\n",
    "X_test = betterVect.transform(df_test['lemma'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "da4dfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train_vect\n",
    "y_train2 = y_train_df\n",
    "X_test2 = tf_vect.transform(df_test['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d76dd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = added_info[['word_count', 'avg_word_length', 'citations', 'rt_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593568c0",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more classification models that is/are able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1fd2598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikechukwu/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7493678887484198"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d263979",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikechukwu/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8644121365360303"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X_train2, y_train2)\n",
    "logreg2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_poly = SVC(kernel=\"poly\")\n",
    "svc_poly.fit(X_train, y_train)\n",
    "svc_poly.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e77d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_poly2 = SVC(kernel=\"poly\")\n",
    "svc_poly2.fit(X_train2, y_train2)\n",
    "svc_poly2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_lm = SVC(kernel=\"linear\")\n",
    "svc_lm.fit(X_train, y_train)\n",
    "svc_lm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b9857779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9171934260429836"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_lm2 = SVC(kernel=\"linear\", probability=True)\n",
    "svc_lm2.fit(X_train2, y_train2)\n",
    "svc_lm2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdBoost = AdaBoostClassifier()\n",
    "AdBoost.fit(X_train, y_train)\n",
    "AdBoost.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdBoost2 = AdaBoostClassifier()\n",
    "AdBoost2.fit(X_train2, y_train2)\n",
    "AdBoost2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "RF.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3fdf1ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993678887484198"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF2 = RandomForestClassifier()\n",
    "RF2.fit(X_train2, y_train2)\n",
    "RF2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "MNB.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee4b5404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6536030341340076"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB2 = MultinomialNB()\n",
    "MNB2.fit(X_train2, y_train2)\n",
    "MNB2.score(X_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "89c73e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = logreg2.predict_proba(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9638db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector = svc_lm2.predict_proba(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96da6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RF2.predict_proba(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "138ea54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['classA', 'classB', 'classC', 'classD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25acbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_df = pd.DataFrame(logistic)\n",
    "logistic_df.columns = classes\n",
    "\n",
    "support_vector_df = pd.DataFrame(support_vector)\n",
    "support_vector_df.columns = classes\n",
    "\n",
    "random_forest_df = pd.DataFrame(random_forest)\n",
    "random_forest_df.columns = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "23836d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_data = pd.DataFrame()\n",
    "\n",
    "interim_data['classA'] = logistic_df['classA'] + support_vector_df['classA'] + random_forest_df['classA']\n",
    "interim_data['classB'] = logistic_df['classB'] + support_vector_df['classB'] + random_forest_df['classB']\n",
    "interim_data['classC'] = logistic_df['classC'] + support_vector_df['classC'] + random_forest_df['classC']\n",
    "interim_data['classD'] = logistic_df['classD'] + support_vector_df['classD'] + random_forest_df['classD']\n",
    "\n",
    "interim = pd.concat([interim_data, addition], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dbce8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train, m_val, m_y_train, m_y_val = train_test_split(interim, y_train2, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "33f51a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikechukwu/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9756637168141593"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_log_clf = LogisticRegression()\n",
    "m_log_clf.fit(m_train, m_y_train)\n",
    "m_log_clf.score(m_val, m_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1ca30745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9759797724399494"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_svc_lm2 = SVC(kernel=\"linear\")\n",
    "m_svc_lm2.fit(m_train, m_y_train)\n",
    "m_svc_lm2.score(m_val, m_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b654c2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97724399494311"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_RF2 = RandomForestClassifier()\n",
    "m_RF2.fit(m_train, m_y_train)\n",
    "m_RF2.score(m_val, m_y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb8078",
   "metadata": {},
   "source": [
    "### Prepare for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ec9234b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_addition = df_test.copy()\n",
    "def count_words(word):\n",
    "    word_list = word.split(\" \")\n",
    "    return len(word_list)\n",
    "\n",
    "test_addition['word_count'] = test_addition['message'].apply(count_words)\n",
    "\n",
    "def avg_word_length(word):\n",
    "    string_length =  len(word)\n",
    "    word_list = word.split(\" \")\n",
    "    word_count = len(word_list)\n",
    "    return string_length/word_count\n",
    "\n",
    "test_addition['avg_word_length'] = test_addition['message'].apply(avg_word_length)\n",
    "\n",
    "def count_citations(word):\n",
    "    word_list = word.split(\" \")\n",
    "    count = 0 \n",
    "    \n",
    "    for word in word_list:\n",
    "        if word == \"urlweb\":\n",
    "            count = count + 1\n",
    "            \n",
    "    return count            \n",
    "\n",
    "test_addition['citations'] = test_addition['message'].apply(count_citations)\n",
    "\n",
    "def count_retweets(word):\n",
    "    word_list = word.split(\" \")\n",
    "    rt_count = 0\n",
    "    for word in word_list:\n",
    "        if word == 'rt':\n",
    "            rt_count = rt_count + 1\n",
    "    return rt_count\n",
    "\n",
    "test_addition['rt_count'] = test_addition['message'].apply(count_retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3ba314c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>citations</th>\n",
       "      <th>rt_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>6.105263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  avg_word_length  citations  rt_count\n",
       "0          19         6.105263          1         0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "959a343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_addition = test_addition[['word_count', 'avg_word_length', 'citations', 'rt_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e1dfdf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classA</th>\n",
       "      <th>classB</th>\n",
       "      <th>classC</th>\n",
       "      <th>classD</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>citations</th>\n",
       "      <th>rt_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.090894</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>2.336278</td>\n",
       "      <td>0.440123</td>\n",
       "      <td>20</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118136</td>\n",
       "      <td>0.458887</td>\n",
       "      <td>2.305033</td>\n",
       "      <td>0.117944</td>\n",
       "      <td>20</td>\n",
       "      <td>5.650000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027738</td>\n",
       "      <td>0.165350</td>\n",
       "      <td>2.724315</td>\n",
       "      <td>0.082597</td>\n",
       "      <td>14</td>\n",
       "      <td>8.071429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083101</td>\n",
       "      <td>0.364161</td>\n",
       "      <td>2.437053</td>\n",
       "      <td>0.115686</td>\n",
       "      <td>23</td>\n",
       "      <td>5.608696</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.240293</td>\n",
       "      <td>1.877761</td>\n",
       "      <td>0.703504</td>\n",
       "      <td>0.178442</td>\n",
       "      <td>8</td>\n",
       "      <td>9.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>0.321340</td>\n",
       "      <td>0.304039</td>\n",
       "      <td>1.726491</td>\n",
       "      <td>0.648130</td>\n",
       "      <td>15</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10542</th>\n",
       "      <td>0.109459</td>\n",
       "      <td>0.111651</td>\n",
       "      <td>2.430984</td>\n",
       "      <td>0.347906</td>\n",
       "      <td>19</td>\n",
       "      <td>6.157895</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10543</th>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.634892</td>\n",
       "      <td>0.998445</td>\n",
       "      <td>1.293533</td>\n",
       "      <td>16</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10544</th>\n",
       "      <td>0.183798</td>\n",
       "      <td>2.536068</td>\n",
       "      <td>0.268651</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>16</td>\n",
       "      <td>6.375000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10545</th>\n",
       "      <td>0.481632</td>\n",
       "      <td>0.407361</td>\n",
       "      <td>1.832884</td>\n",
       "      <td>0.278123</td>\n",
       "      <td>23</td>\n",
       "      <td>5.695652</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         classA    classB    classC    classD  word_count  avg_word_length  \\\n",
       "0      0.090894  0.132705  2.336278  0.440123          20         5.200000   \n",
       "1      0.118136  0.458887  2.305033  0.117944          20         5.650000   \n",
       "2      0.027738  0.165350  2.724315  0.082597          14         8.071429   \n",
       "3      0.083101  0.364161  2.437053  0.115686          23         5.608696   \n",
       "4      0.240293  1.877761  0.703504  0.178442           8         9.125000   \n",
       "...         ...       ...       ...       ...         ...              ...   \n",
       "10541  0.321340  0.304039  1.726491  0.648130          15         8.000000   \n",
       "10542  0.109459  0.111651  2.430984  0.347906          19         6.157895   \n",
       "10543  0.073130  0.634892  0.998445  1.293533          16         7.250000   \n",
       "10544  0.183798  2.536068  0.268651  0.011483          16         6.375000   \n",
       "10545  0.481632  0.407361  1.832884  0.278123          23         5.695652   \n",
       "\n",
       "       citations  rt_count  \n",
       "0              1         0  \n",
       "1              1         0  \n",
       "2              1         0  \n",
       "3              0         0  \n",
       "4              0         1  \n",
       "...          ...       ...  \n",
       "10541          0         1  \n",
       "10542          1         0  \n",
       "10543          1         1  \n",
       "10544          0         1  \n",
       "10545          0         1  \n",
       "\n",
       "[10546 rows x 8 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logistic = logreg2.predict_proba(X_test2)\n",
    "support_vector = svc_lm2.predict_proba(X_test2)\n",
    "random_forest = RF2.predict_proba(X_test2)\n",
    "\n",
    "classes = ['classA', 'classB', 'classC', 'classD']\n",
    "\n",
    "logistic_df = pd.DataFrame(logistic)\n",
    "logistic_df.columns = classes\n",
    "\n",
    "support_vector_df = pd.DataFrame(support_vector)\n",
    "support_vector_df.columns = classes\n",
    "\n",
    "random_forest_df = pd.DataFrame(random_forest)\n",
    "random_forest_df.columns = classes\n",
    "\n",
    "interim_data = pd.DataFrame()\n",
    "\n",
    "interim_data['classA'] = logistic_df['classA'] + support_vector_df['classA'] + random_forest_df['classA']\n",
    "interim_data['classB'] = logistic_df['classB'] + support_vector_df['classB'] + random_forest_df['classB']\n",
    "interim_data['classC'] = logistic_df['classC'] + support_vector_df['classC'] + random_forest_df['classC']\n",
    "interim_data['classD'] = logistic_df['classD'] + support_vector_df['classD'] + random_forest_df['classD']\n",
    "\n",
    "interim = pd.concat([interim_data, test_addition], axis=1)\n",
    "interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a6a6dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m_log_clf.predict(interim)\n",
    "\n",
    "\n",
    "df_CSV = pd.DataFrame({\"tweetid\": df_test['tweetid'].values,\n",
    "                   \"sentiment\": predictions,\n",
    "                  })\n",
    "\n",
    "df_CSV.to_csv(\"Team6_sample_MNB_ProT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036b84f",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee55a1d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c112a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
